---
{"dg-publish":true,"permalink":"/probability-theory/","title":"Probability Theory","created":"2022-09-25T14:40:01","updated":"2022-12-08T22:28:42"}
---

> [!meta]-
sup:: [[Math\|Math]]  
state:: done

# Probability Theory

## Special Random Variables

| Distribution                    | Notation                                                | Parameters                                                  | CDF                                                     | PMF/PDF                                                    | Mean                                                        | Variance                                                   | MGF                                                     |
| ------------------------------- | ------------------------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------- | ---------------------------------------------------------- | ----------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------- |
| [[Uniform Distribution\|Uniform Distribution]]        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline n-link naked

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
    | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline n-link naked

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
     | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Uniform Distribution

The uniform distribution over $[a,b]$ has

- Notation
    - $\mathcal{U}(a,b)$ ^nota
- Parameters
    - $a,b\in\R$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases}0,  \quad & x<a,\\ \frac{x-a}{b-a}, &a\le x\le b,\\ 1,  & x>b.\end{cases}$ ^cdf
- [[Probability Density Function\|PDF]]
    - $f(x) = \begin{cases} \frac{1}{b-a}, \quad & a\le x\le b,\\ 0, & \text{otherwise.}\end{cases}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{a+b}{2}$ ^mean
- [[Variance\|Variance]]
    - $\frac{(b-a)^{2}}{12}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\displaystyle \frac{e^{tb} - e^{ta}}{(b-a)t}$ ^mgf


</div></div>
     |
| [[Bernoulli Distribution\|Bernoulli Distribution]]      | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup::[[Probability Theory\|Probability Theory]]
state:: done

# Bernoulli Distribution

A [[Random Variable\|Random Variable]] is of Bernoulli distribution if its range is $\{ 0,1 \}$.

- Parameters
    - $p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(n) = \begin{cases} p, \quad & n=1, ,\\ q\coloneqq 1-p, & n=0.\end{cases}$ ^pdf
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases} 0, \quad & x<0,\\ q = 1-p, & 0\le x\le 1,\\ 1, & x >1. \end{cases}$ ^cdf
- [[Expectation\|Mean]]
    - $p$ ^mean
- [[Variance\|Variance]]
    - $pq$ ^var
- [[Moment Generating Function\|MGF]]
    - $q + pe^{t}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline n-link naked

</div>



> [!meta]-  
sup::[[Probability Theory\|Probability Theory]]
state:: done

# Bernoulli Distribution

A [[Random Variable\|Random Variable]] is of Bernoulli distribution if its range is $\{ 0,1 \}$.

- Parameters
    - $p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(n) = \begin{cases} p, \quad & n=1, ,\\ q\coloneqq 1-p, & n=0.\end{cases}$ ^pdf
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases} 0, \quad & x<0,\\ q = 1-p, & 0\le x\le 1,\\ 1, & x >1. \end{cases}$ ^cdf
- [[Expectation\|Mean]]
    - $p$ ^mean
- [[Variance\|Variance]]
    - $pq$ ^var
- [[Moment Generating Function\|MGF]]
    - $q + pe^{t}$ ^mgf


</div></div>
   | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup::[[Probability Theory\|Probability Theory]]
state:: done

# Bernoulli Distribution

A [[Random Variable\|Random Variable]] is of Bernoulli distribution if its range is $\{ 0,1 \}$.

- Parameters
    - $p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(n) = \begin{cases} p, \quad & n=1, ,\\ q\coloneqq 1-p, & n=0.\end{cases}$ ^pdf
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases} 0, \quad & x<0,\\ q = 1-p, & 0\le x\le 1,\\ 1, & x >1. \end{cases}$ ^cdf
- [[Expectation\|Mean]]
    - $p$ ^mean
- [[Variance\|Variance]]
    - $pq$ ^var
- [[Moment Generating Function\|MGF]]
    - $q + pe^{t}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup::[[Probability Theory\|Probability Theory]]
state:: done

# Bernoulli Distribution

A [[Random Variable\|Random Variable]] is of Bernoulli distribution if its range is $\{ 0,1 \}$.

- Parameters
    - $p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(n) = \begin{cases} p, \quad & n=1, ,\\ q\coloneqq 1-p, & n=0.\end{cases}$ ^pdf
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases} 0, \quad & x<0,\\ q = 1-p, & 0\le x\le 1,\\ 1, & x >1. \end{cases}$ ^cdf
- [[Expectation\|Mean]]
    - $p$ ^mean
- [[Variance\|Variance]]
    - $pq$ ^var
- [[Moment Generating Function\|MGF]]
    - $q + pe^{t}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup::[[Probability Theory\|Probability Theory]]
state:: done

# Bernoulli Distribution

A [[Random Variable\|Random Variable]] is of Bernoulli distribution if its range is $\{ 0,1 \}$.

- Parameters
    - $p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(n) = \begin{cases} p, \quad & n=1, ,\\ q\coloneqq 1-p, & n=0.\end{cases}$ ^pdf
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases} 0, \quad & x<0,\\ q = 1-p, & 0\le x\le 1,\\ 1, & x >1. \end{cases}$ ^cdf
- [[Expectation\|Mean]]
    - $p$ ^mean
- [[Variance\|Variance]]
    - $pq$ ^var
- [[Moment Generating Function\|MGF]]
    - $q + pe^{t}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup::[[Probability Theory\|Probability Theory]]
state:: done

# Bernoulli Distribution

A [[Random Variable\|Random Variable]] is of Bernoulli distribution if its range is $\{ 0,1 \}$.

- Parameters
    - $p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(n) = \begin{cases} p, \quad & n=1, ,\\ q\coloneqq 1-p, & n=0.\end{cases}$ ^pdf
- [[Cumulative Distribution Function\|CDF]]
    - $F(x) = \begin{cases} 0, \quad & x<0,\\ q = 1-p, & 0\le x\le 1,\\ 1, & x >1. \end{cases}$ ^cdf
- [[Expectation\|Mean]]
    - $p$ ^mean
- [[Variance\|Variance]]
    - $pq$ ^var
- [[Moment Generating Function\|MGF]]
    - $q + pe^{t}$ ^mgf


</div></div>
   |
| [[Binomial Distribution\|Binomial Distribution]]       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline n-link naked

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Binomial Distribution

A discrete [[Random Variable\|Random Variable]] is of binomial distribution if its range is $\mathbb{N}$ and

- Notation
    - $B(n,p)$ ^nota
- Parameters
    - $n \in \mathbb{N}, p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(k) = {n \choose k} p^{k}(1-p)^{n-k}$ ^pdf
- [[Expectation\|Mean]]
    - $np$ ^mean
- [[Variance\|Variance]]
    - $npq$ ^var
- [[Moment Generating Function\|MGF]]
    - $(q + pe^{t})^{n}$ ^mgf


</div></div>
   | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Binomial Distribution

A discrete [[Random Variable\|Random Variable]] is of binomial distribution if its range is $\mathbb{N}$ and

- Notation
    - $B(n,p)$ ^nota
- Parameters
    - $n \in \mathbb{N}, p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(k) = {n \choose k} p^{k}(1-p)^{n-k}$ ^pdf
- [[Expectation\|Mean]]
    - $np$ ^mean
- [[Variance\|Variance]]
    - $npq$ ^var
- [[Moment Generating Function\|MGF]]
    - $(q + pe^{t})^{n}$ ^mgf


</div></div>
       | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Binomial Distribution

A discrete [[Random Variable\|Random Variable]] is of binomial distribution if its range is $\mathbb{N}$ and

- Notation
    - $B(n,p)$ ^nota
- Parameters
    - $n \in \mathbb{N}, p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(k) = {n \choose k} p^{k}(1-p)^{n-k}$ ^pdf
- [[Expectation\|Mean]]
    - $np$ ^mean
- [[Variance\|Variance]]
    - $npq$ ^var
- [[Moment Generating Function\|MGF]]
    - $(q + pe^{t})^{n}$ ^mgf


</div></div>
       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Binomial Distribution

A discrete [[Random Variable\|Random Variable]] is of binomial distribution if its range is $\mathbb{N}$ and

- Notation
    - $B(n,p)$ ^nota
- Parameters
    - $n \in \mathbb{N}, p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(k) = {n \choose k} p^{k}(1-p)^{n-k}$ ^pdf
- [[Expectation\|Mean]]
    - $np$ ^mean
- [[Variance\|Variance]]
    - $npq$ ^var
- [[Moment Generating Function\|MGF]]
    - $(q + pe^{t})^{n}$ ^mgf


</div></div>
       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Binomial Distribution

A discrete [[Random Variable\|Random Variable]] is of binomial distribution if its range is $\mathbb{N}$ and

- Notation
    - $B(n,p)$ ^nota
- Parameters
    - $n \in \mathbb{N}, p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(k) = {n \choose k} p^{k}(1-p)^{n-k}$ ^pdf
- [[Expectation\|Mean]]
    - $np$ ^mean
- [[Variance\|Variance]]
    - $npq$ ^var
- [[Moment Generating Function\|MGF]]
    - $(q + pe^{t})^{n}$ ^mgf


</div></div>
       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Binomial Distribution

A discrete [[Random Variable\|Random Variable]] is of binomial distribution if its range is $\mathbb{N}$ and

- Notation
    - $B(n,p)$ ^nota
- Parameters
    - $n \in \mathbb{N}, p\in[0,1]$ ^para
- [[Probability Mass Function\|PMF]]
    - $p(k) = {n \choose k} p^{k}(1-p)^{n-k}$ ^pdf
- [[Expectation\|Mean]]
    - $np$ ^mean
- [[Variance\|Variance]]
    - $npq$ ^var
- [[Moment Generating Function\|MGF]]
    - $(q + pe^{t})^{n}$ ^mgf


</div></div>
    |
| [[Poisson Distribution\|Poisson Distribution]]        | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Poisson Distribution

A discrete [[Random Variable\|Random Variable]] is of Poisson distribution if its range is $\mathbb{N}$, and

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(n) = e^{-\lambda} \frac{\lambda ^{n}}{n!}$ ^pdf
    - The important part is $\lambda ^{n} / n!$; $e^{-\lambda}$ is just a constant to make if sum to 1
    - Remember it as the **expansion** of the exponential
- [[Expectation\|Mean]]
    - $\lambda$ ^mean
- [[Variance\|Variance]]
    - $\lambda$ ^var
- [[Moment Generating Function\|MGF]]
    - $\exp(\lambda(e^{t}-1))$ ^mgf

## Poisson Distribution as Approximation to Binomial Distribution

When $n$ is large and $p$ is small, Poisson distribution can be an approximation of a [[Binomial Distribution\|Binomial Distribution]]. This is because for $X\sim \operatorname{Binom}(n,p)$, we have
$
P(X = k) = {n \choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!}\left( \frac{np}{n} \right)^{k}\left( 1 - \frac{np}{n} \right)^{n}\left( 1 - p \right)^{-k}
$
When $n$ is large and $p$ is small, we have
$
\frac{n(n-1)\dots(n-k+1)}{n^{k}} \approx 1, \quad \left( 1 - \frac{np}{n} \right)^{n} \approx e^{-np}, \quad (1-p)^{-k} \approx 1.
$

Therefore, we have
$
P(X = k) \approx e^{-np} \frac{(np)^{k}}{k!},
$
i.e., $X \dot{\sim} \operatorname{Poisson}(np)$.

- [!] The Poisson approximation result can be shown to be valid under even more general conditions than those so far mentioned. For instance, suppose that n independent trials are to be performed, with the ith trial resulting in a success with probability $p_i, i = 1,...,n$. Then it can be shown that if n is large and each $p_i$ is small, then the number of successful trials is approximately Poisson distributed with mean equal to $\sum^n_{i=1}p_i$. In fact, this result will sometimes remain true even when the trials are not independent, provided that their dependence is “weak.” For instance, consider the following example.


</div></div>
        | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Poisson Distribution

A discrete [[Random Variable\|Random Variable]] is of Poisson distribution if its range is $\mathbb{N}$, and

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(n) = e^{-\lambda} \frac{\lambda ^{n}}{n!}$ ^pdf
    - The important part is $\lambda ^{n} / n!$; $e^{-\lambda}$ is just a constant to make if sum to 1
    - Remember it as the **expansion** of the exponential
- [[Expectation\|Mean]]
    - $\lambda$ ^mean
- [[Variance\|Variance]]
    - $\lambda$ ^var
- [[Moment Generating Function\|MGF]]
    - $\exp(\lambda(e^{t}-1))$ ^mgf

## Poisson Distribution as Approximation to Binomial Distribution

When $n$ is large and $p$ is small, Poisson distribution can be an approximation of a [[Binomial Distribution\|Binomial Distribution]]. This is because for $X\sim \operatorname{Binom}(n,p)$, we have
$
P(X = k) = {n \choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!}\left( \frac{np}{n} \right)^{k}\left( 1 - \frac{np}{n} \right)^{n}\left( 1 - p \right)^{-k}
$
When $n$ is large and $p$ is small, we have
$
\frac{n(n-1)\dots(n-k+1)}{n^{k}} \approx 1, \quad \left( 1 - \frac{np}{n} \right)^{n} \approx e^{-np}, \quad (1-p)^{-k} \approx 1.
$

Therefore, we have
$
P(X = k) \approx e^{-np} \frac{(np)^{k}}{k!},
$
i.e., $X \dot{\sim} \operatorname{Poisson}(np)$.

- [!] The Poisson approximation result can be shown to be valid under even more general conditions than those so far mentioned. For instance, suppose that n independent trials are to be performed, with the ith trial resulting in a success with probability $p_i, i = 1,...,n$. Then it can be shown that if n is large and each $p_i$ is small, then the number of successful trials is approximately Poisson distributed with mean equal to $\sum^n_{i=1}p_i$. In fact, this result will sometimes remain true even when the trials are not independent, provided that their dependence is “weak.” For instance, consider the following example.


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Poisson Distribution

A discrete [[Random Variable\|Random Variable]] is of Poisson distribution if its range is $\mathbb{N}$, and

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(n) = e^{-\lambda} \frac{\lambda ^{n}}{n!}$ ^pdf
    - The important part is $\lambda ^{n} / n!$; $e^{-\lambda}$ is just a constant to make if sum to 1
    - Remember it as the **expansion** of the exponential
- [[Expectation\|Mean]]
    - $\lambda$ ^mean
- [[Variance\|Variance]]
    - $\lambda$ ^var
- [[Moment Generating Function\|MGF]]
    - $\exp(\lambda(e^{t}-1))$ ^mgf

## Poisson Distribution as Approximation to Binomial Distribution

When $n$ is large and $p$ is small, Poisson distribution can be an approximation of a [[Binomial Distribution\|Binomial Distribution]]. This is because for $X\sim \operatorname{Binom}(n,p)$, we have
$
P(X = k) = {n \choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!}\left( \frac{np}{n} \right)^{k}\left( 1 - \frac{np}{n} \right)^{n}\left( 1 - p \right)^{-k}
$
When $n$ is large and $p$ is small, we have
$
\frac{n(n-1)\dots(n-k+1)}{n^{k}} \approx 1, \quad \left( 1 - \frac{np}{n} \right)^{n} \approx e^{-np}, \quad (1-p)^{-k} \approx 1.
$

Therefore, we have
$
P(X = k) \approx e^{-np} \frac{(np)^{k}}{k!},
$
i.e., $X \dot{\sim} \operatorname{Poisson}(np)$.

- [!] The Poisson approximation result can be shown to be valid under even more general conditions than those so far mentioned. For instance, suppose that n independent trials are to be performed, with the ith trial resulting in a success with probability $p_i, i = 1,...,n$. Then it can be shown that if n is large and each $p_i$ is small, then the number of successful trials is approximately Poisson distributed with mean equal to $\sum^n_{i=1}p_i$. In fact, this result will sometimes remain true even when the trials are not independent, provided that their dependence is “weak.” For instance, consider the following example.


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Poisson Distribution

A discrete [[Random Variable\|Random Variable]] is of Poisson distribution if its range is $\mathbb{N}$, and

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(n) = e^{-\lambda} \frac{\lambda ^{n}}{n!}$ ^pdf
    - The important part is $\lambda ^{n} / n!$; $e^{-\lambda}$ is just a constant to make if sum to 1
    - Remember it as the **expansion** of the exponential
- [[Expectation\|Mean]]
    - $\lambda$ ^mean
- [[Variance\|Variance]]
    - $\lambda$ ^var
- [[Moment Generating Function\|MGF]]
    - $\exp(\lambda(e^{t}-1))$ ^mgf

## Poisson Distribution as Approximation to Binomial Distribution

When $n$ is large and $p$ is small, Poisson distribution can be an approximation of a [[Binomial Distribution\|Binomial Distribution]]. This is because for $X\sim \operatorname{Binom}(n,p)$, we have
$
P(X = k) = {n \choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!}\left( \frac{np}{n} \right)^{k}\left( 1 - \frac{np}{n} \right)^{n}\left( 1 - p \right)^{-k}
$
When $n$ is large and $p$ is small, we have
$
\frac{n(n-1)\dots(n-k+1)}{n^{k}} \approx 1, \quad \left( 1 - \frac{np}{n} \right)^{n} \approx e^{-np}, \quad (1-p)^{-k} \approx 1.
$

Therefore, we have
$
P(X = k) \approx e^{-np} \frac{(np)^{k}}{k!},
$
i.e., $X \dot{\sim} \operatorname{Poisson}(np)$.

- [!] The Poisson approximation result can be shown to be valid under even more general conditions than those so far mentioned. For instance, suppose that n independent trials are to be performed, with the ith trial resulting in a success with probability $p_i, i = 1,...,n$. Then it can be shown that if n is large and each $p_i$ is small, then the number of successful trials is approximately Poisson distributed with mean equal to $\sum^n_{i=1}p_i$. In fact, this result will sometimes remain true even when the trials are not independent, provided that their dependence is “weak.” For instance, consider the following example.


</div></div>
        | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Poisson Distribution

A discrete [[Random Variable\|Random Variable]] is of Poisson distribution if its range is $\mathbb{N}$, and

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(n) = e^{-\lambda} \frac{\lambda ^{n}}{n!}$ ^pdf
    - The important part is $\lambda ^{n} / n!$; $e^{-\lambda}$ is just a constant to make if sum to 1
    - Remember it as the **expansion** of the exponential
- [[Expectation\|Mean]]
    - $\lambda$ ^mean
- [[Variance\|Variance]]
    - $\lambda$ ^var
- [[Moment Generating Function\|MGF]]
    - $\exp(\lambda(e^{t}-1))$ ^mgf

## Poisson Distribution as Approximation to Binomial Distribution

When $n$ is large and $p$ is small, Poisson distribution can be an approximation of a [[Binomial Distribution\|Binomial Distribution]]. This is because for $X\sim \operatorname{Binom}(n,p)$, we have
$
P(X = k) = {n \choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!}\left( \frac{np}{n} \right)^{k}\left( 1 - \frac{np}{n} \right)^{n}\left( 1 - p \right)^{-k}
$
When $n$ is large and $p$ is small, we have
$
\frac{n(n-1)\dots(n-k+1)}{n^{k}} \approx 1, \quad \left( 1 - \frac{np}{n} \right)^{n} \approx e^{-np}, \quad (1-p)^{-k} \approx 1.
$

Therefore, we have
$
P(X = k) \approx e^{-np} \frac{(np)^{k}}{k!},
$
i.e., $X \dot{\sim} \operatorname{Poisson}(np)$.

- [!] The Poisson approximation result can be shown to be valid under even more general conditions than those so far mentioned. For instance, suppose that n independent trials are to be performed, with the ith trial resulting in a success with probability $p_i, i = 1,...,n$. Then it can be shown that if n is large and each $p_i$ is small, then the number of successful trials is approximately Poisson distributed with mean equal to $\sum^n_{i=1}p_i$. In fact, this result will sometimes remain true even when the trials are not independent, provided that their dependence is “weak.” For instance, consider the following example.


</div></div>
     |
| [[Exponential Distribution\|Exponential Distribution]]    | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Exponential Distribution

A continuous [[Random Variable\|Random Variable]] is of exponential distribution if it has

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(n) = \begin{cases} \lambda e^{-\lambda x}, \quad &x \ge 0,\\ 0, & x< 0 \end{cases}$ ^pdf
    - The important part is $e^{-\lambda x}$; $\lambda$ is just a constant to make if integral to 1
- [[Cumulative Distribution Function\|CDF]]
    - $1 - e^{-\lambda x}$ ^cdf
- [[Expectation\|Mean]]
    - $\frac{1}{\lambda}$ ^mean
- [[Variance\|Variance]]
    - $\frac{1}{\lambda^{2}}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\lambda /(\lambda - t), \quad t < \lambda$ ^mgf

By the uniqueness of [[Moment Generating Function\|MGF]], exponential distribution is [[Gamma Distribution\|Gamma Distribution]] with parameter $(1,\lambda)$.

## Exponential Distribution and Poisson Distribution

Exponential distribution and [[Poisson Distribution\|Poisson Distribution]] are similar in many ways. Actually, **the waiting times for poisson distribution is an exponential distribution with parameter lambda**. Actually, let $Y$ be the waiting time, let $\lambda$ be the average number of arrivals per time. For $t \ge 0$, let $X \sim \operatorname{Poisson}(\lambda t)$. We know that
$
P(Y > t) = P(X = 0) = e^{-\lambda t} \frac{(\lambda t)^{0}}{0!} = e^{-\lambda t}.
$

Then $P(Y \le t) = 1 - e^{-\lambda t}$, i.e., $Y \sim \exp(\lambda)$.

## Memoryless

We say a nonnegative [[Random Variable\|Random Variable]] $X$ is memoryless, if for $n,k\ge{0}$
$
P(X \ge n+k | X \ge n) = P(X \ge k).
$

An exponential random variable is memoryless because
$
P(X \ge n+k | X\ge n) = \frac{e^{-\lambda(n+k)}}{e^{-\lambda n}} = e^{-\lambda k} = P(x \ge k).
$


</div></div>
    | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# n-link inline naked

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Exponential Distribution

A continuous [[Random Variable\|Random Variable]] is of exponential distribution if it has

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(n) = \begin{cases} \lambda e^{-\lambda x}, \quad &x \ge 0,\\ 0, & x< 0 \end{cases}$ ^pdf
    - The important part is $e^{-\lambda x}$; $\lambda$ is just a constant to make if integral to 1
- [[Cumulative Distribution Function\|CDF]]
    - $1 - e^{-\lambda x}$ ^cdf
- [[Expectation\|Mean]]
    - $\frac{1}{\lambda}$ ^mean
- [[Variance\|Variance]]
    - $\frac{1}{\lambda^{2}}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\lambda /(\lambda - t), \quad t < \lambda$ ^mgf

By the uniqueness of [[Moment Generating Function\|MGF]], exponential distribution is [[Gamma Distribution\|Gamma Distribution]] with parameter $(1,\lambda)$.

## Exponential Distribution and Poisson Distribution

Exponential distribution and [[Poisson Distribution\|Poisson Distribution]] are similar in many ways. Actually, **the waiting times for poisson distribution is an exponential distribution with parameter lambda**. Actually, let $Y$ be the waiting time, let $\lambda$ be the average number of arrivals per time. For $t \ge 0$, let $X \sim \operatorname{Poisson}(\lambda t)$. We know that
$
P(Y > t) = P(X = 0) = e^{-\lambda t} \frac{(\lambda t)^{0}}{0!} = e^{-\lambda t}.
$

Then $P(Y \le t) = 1 - e^{-\lambda t}$, i.e., $Y \sim \exp(\lambda)$.

## Memoryless

We say a nonnegative [[Random Variable\|Random Variable]] $X$ is memoryless, if for $n,k\ge{0}$
$
P(X \ge n+k | X \ge n) = P(X \ge k).
$

An exponential random variable is memoryless because
$
P(X \ge n+k | X\ge n) = \frac{e^{-\lambda(n+k)}}{e^{-\lambda n}} = e^{-\lambda k} = P(x \ge k).
$


</div></div>
 | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Exponential Distribution

A continuous [[Random Variable\|Random Variable]] is of exponential distribution if it has

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(n) = \begin{cases} \lambda e^{-\lambda x}, \quad &x \ge 0,\\ 0, & x< 0 \end{cases}$ ^pdf
    - The important part is $e^{-\lambda x}$; $\lambda$ is just a constant to make if integral to 1
- [[Cumulative Distribution Function\|CDF]]
    - $1 - e^{-\lambda x}$ ^cdf
- [[Expectation\|Mean]]
    - $\frac{1}{\lambda}$ ^mean
- [[Variance\|Variance]]
    - $\frac{1}{\lambda^{2}}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\lambda /(\lambda - t), \quad t < \lambda$ ^mgf

By the uniqueness of [[Moment Generating Function\|MGF]], exponential distribution is [[Gamma Distribution\|Gamma Distribution]] with parameter $(1,\lambda)$.

## Exponential Distribution and Poisson Distribution

Exponential distribution and [[Poisson Distribution\|Poisson Distribution]] are similar in many ways. Actually, **the waiting times for poisson distribution is an exponential distribution with parameter lambda**. Actually, let $Y$ be the waiting time, let $\lambda$ be the average number of arrivals per time. For $t \ge 0$, let $X \sim \operatorname{Poisson}(\lambda t)$. We know that
$
P(Y > t) = P(X = 0) = e^{-\lambda t} \frac{(\lambda t)^{0}}{0!} = e^{-\lambda t}.
$

Then $P(Y \le t) = 1 - e^{-\lambda t}$, i.e., $Y \sim \exp(\lambda)$.

## Memoryless

We say a nonnegative [[Random Variable\|Random Variable]] $X$ is memoryless, if for $n,k\ge{0}$
$
P(X \ge n+k | X \ge n) = P(X \ge k).
$

An exponential random variable is memoryless because
$
P(X \ge n+k | X\ge n) = \frac{e^{-\lambda(n+k)}}{e^{-\lambda n}} = e^{-\lambda k} = P(x \ge k).
$


</div></div>
    | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Exponential Distribution

A continuous [[Random Variable\|Random Variable]] is of exponential distribution if it has

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(n) = \begin{cases} \lambda e^{-\lambda x}, \quad &x \ge 0,\\ 0, & x< 0 \end{cases}$ ^pdf
    - The important part is $e^{-\lambda x}$; $\lambda$ is just a constant to make if integral to 1
- [[Cumulative Distribution Function\|CDF]]
    - $1 - e^{-\lambda x}$ ^cdf
- [[Expectation\|Mean]]
    - $\frac{1}{\lambda}$ ^mean
- [[Variance\|Variance]]
    - $\frac{1}{\lambda^{2}}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\lambda /(\lambda - t), \quad t < \lambda$ ^mgf

By the uniqueness of [[Moment Generating Function\|MGF]], exponential distribution is [[Gamma Distribution\|Gamma Distribution]] with parameter $(1,\lambda)$.

## Exponential Distribution and Poisson Distribution

Exponential distribution and [[Poisson Distribution\|Poisson Distribution]] are similar in many ways. Actually, **the waiting times for poisson distribution is an exponential distribution with parameter lambda**. Actually, let $Y$ be the waiting time, let $\lambda$ be the average number of arrivals per time. For $t \ge 0$, let $X \sim \operatorname{Poisson}(\lambda t)$. We know that
$
P(Y > t) = P(X = 0) = e^{-\lambda t} \frac{(\lambda t)^{0}}{0!} = e^{-\lambda t}.
$

Then $P(Y \le t) = 1 - e^{-\lambda t}$, i.e., $Y \sim \exp(\lambda)$.

## Memoryless

We say a nonnegative [[Random Variable\|Random Variable]] $X$ is memoryless, if for $n,k\ge{0}$
$
P(X \ge n+k | X \ge n) = P(X \ge k).
$

An exponential random variable is memoryless because
$
P(X \ge n+k | X\ge n) = \frac{e^{-\lambda(n+k)}}{e^{-\lambda n}} = e^{-\lambda k} = P(x \ge k).
$


</div></div>
    | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Exponential Distribution

A continuous [[Random Variable\|Random Variable]] is of exponential distribution if it has

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(n) = \begin{cases} \lambda e^{-\lambda x}, \quad &x \ge 0,\\ 0, & x< 0 \end{cases}$ ^pdf
    - The important part is $e^{-\lambda x}$; $\lambda$ is just a constant to make if integral to 1
- [[Cumulative Distribution Function\|CDF]]
    - $1 - e^{-\lambda x}$ ^cdf
- [[Expectation\|Mean]]
    - $\frac{1}{\lambda}$ ^mean
- [[Variance\|Variance]]
    - $\frac{1}{\lambda^{2}}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\lambda /(\lambda - t), \quad t < \lambda$ ^mgf

By the uniqueness of [[Moment Generating Function\|MGF]], exponential distribution is [[Gamma Distribution\|Gamma Distribution]] with parameter $(1,\lambda)$.

## Exponential Distribution and Poisson Distribution

Exponential distribution and [[Poisson Distribution\|Poisson Distribution]] are similar in many ways. Actually, **the waiting times for poisson distribution is an exponential distribution with parameter lambda**. Actually, let $Y$ be the waiting time, let $\lambda$ be the average number of arrivals per time. For $t \ge 0$, let $X \sim \operatorname{Poisson}(\lambda t)$. We know that
$
P(Y > t) = P(X = 0) = e^{-\lambda t} \frac{(\lambda t)^{0}}{0!} = e^{-\lambda t}.
$

Then $P(Y \le t) = 1 - e^{-\lambda t}$, i.e., $Y \sim \exp(\lambda)$.

## Memoryless

We say a nonnegative [[Random Variable\|Random Variable]] $X$ is memoryless, if for $n,k\ge{0}$
$
P(X \ge n+k | X \ge n) = P(X \ge k).
$

An exponential random variable is memoryless because
$
P(X \ge n+k | X\ge n) = \frac{e^{-\lambda(n+k)}}{e^{-\lambda n}} = e^{-\lambda k} = P(x \ge k).
$


</div></div>
    | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Exponential Distribution

A continuous [[Random Variable\|Random Variable]] is of exponential distribution if it has

- Parameters
    - $\lambda > 0$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(n) = \begin{cases} \lambda e^{-\lambda x}, \quad &x \ge 0,\\ 0, & x< 0 \end{cases}$ ^pdf
    - The important part is $e^{-\lambda x}$; $\lambda$ is just a constant to make if integral to 1
- [[Cumulative Distribution Function\|CDF]]
    - $1 - e^{-\lambda x}$ ^cdf
- [[Expectation\|Mean]]
    - $\frac{1}{\lambda}$ ^mean
- [[Variance\|Variance]]
    - $\frac{1}{\lambda^{2}}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\lambda /(\lambda - t), \quad t < \lambda$ ^mgf

By the uniqueness of [[Moment Generating Function\|MGF]], exponential distribution is [[Gamma Distribution\|Gamma Distribution]] with parameter $(1,\lambda)$.

## Exponential Distribution and Poisson Distribution

Exponential distribution and [[Poisson Distribution\|Poisson Distribution]] are similar in many ways. Actually, **the waiting times for poisson distribution is an exponential distribution with parameter lambda**. Actually, let $Y$ be the waiting time, let $\lambda$ be the average number of arrivals per time. For $t \ge 0$, let $X \sim \operatorname{Poisson}(\lambda t)$. We know that
$
P(Y > t) = P(X = 0) = e^{-\lambda t} \frac{(\lambda t)^{0}}{0!} = e^{-\lambda t}.
$

Then $P(Y \le t) = 1 - e^{-\lambda t}$, i.e., $Y \sim \exp(\lambda)$.

## Memoryless

We say a nonnegative [[Random Variable\|Random Variable]] $X$ is memoryless, if for $n,k\ge{0}$
$
P(X \ge n+k | X \ge n) = P(X \ge k).
$

An exponential random variable is memoryless because
$
P(X \ge n+k | X\ge n) = \frac{e^{-\lambda(n+k)}}{e^{-\lambda n}} = e^{-\lambda k} = P(x \ge k).
$


</div></div>
 |
| [[Normal Distribution\|Normal Distribution]]         | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline n-link naked

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done  

# Normal Distribution

A [[Random Variable\|Random Variable]] is said to be normally distributed if it has

- Notation
    - $\mathcal{N}(\mu, \sigma^{2})$ ^nota
- Parameters
    - $\mu\in\R, \sigma^{2}\in\R_{+}$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$ ^pdf
    - $(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$
- [[Expectation\|Mean]]
    - $\mu$ ^mean
- [[Variance\|Variance]]
    - $\sigma^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $e^{\mu t + \sigma^{2}t^{2} /2}$ ^mgf

Because of the [[Central Limit Theorem\|Central Limit Theorem]], in practice, many random phenomena obey, at least approximately, a normal probability distribution.

## Properties

- The [[Affine Transformation\|Affine Transformation]] of a normal random variable $X$: $a + BX$ is also a normal random variable
- The sum of independent normal random variables is also a normal random variable
- Hence, if $X\sim \mathcal{N}(\mu,\sigma^{2})$, then $Z = (X - \mu) /\sigma$ is normal with mean 0 and variance 1; $Z$ is said to have a ==standard== or ==unit== normal distribution
    - We write the PDF of a standard normal distribution $\Phi$
- **Symmetry:** $\Phi(-x) = 1 - \Phi(x)$

## Sample Mean and Sample Variance

> [!thm]
>
> If $\{ X _i \}_{i=1}^{n}$ is a sample from a normal population having mean $\mu$ and variance $\sigma^{2}$, then $\overline{X}$ and $S^{2}$ are independent random variables,
> with $\overline{X} \sim \mathcal{N}(\mu,\sigma^{2} /n)$, and $\frac{n-1}{\sigma^{2}}S^{2} \sim \chi^{2}_{n-1}$.
> Then we have $\frac{\overline{X} - \mu}{S /\sqrt{n }} \sim t_{n-1}$.

- [!] This independence of $\overline{X}$ and $S^2$ is a unique property of the normal distribution.


</div></div>
     | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done  

# Normal Distribution

A [[Random Variable\|Random Variable]] is said to be normally distributed if it has

- Notation
    - $\mathcal{N}(\mu, \sigma^{2})$ ^nota
- Parameters
    - $\mu\in\R, \sigma^{2}\in\R_{+}$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$ ^pdf
    - $(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$
- [[Expectation\|Mean]]
    - $\mu$ ^mean
- [[Variance\|Variance]]
    - $\sigma^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $e^{\mu t + \sigma^{2}t^{2} /2}$ ^mgf

Because of the [[Central Limit Theorem\|Central Limit Theorem]], in practice, many random phenomena obey, at least approximately, a normal probability distribution.

## Properties

- The [[Affine Transformation\|Affine Transformation]] of a normal random variable $X$: $a + BX$ is also a normal random variable
- The sum of independent normal random variables is also a normal random variable
- Hence, if $X\sim \mathcal{N}(\mu,\sigma^{2})$, then $Z = (X - \mu) /\sigma$ is normal with mean 0 and variance 1; $Z$ is said to have a ==standard== or ==unit== normal distribution
    - We write the PDF of a standard normal distribution $\Phi$
- **Symmetry:** $\Phi(-x) = 1 - \Phi(x)$

## Sample Mean and Sample Variance

> [!thm]
>
> If $\{ X _i \}_{i=1}^{n}$ is a sample from a normal population having mean $\mu$ and variance $\sigma^{2}$, then $\overline{X}$ and $S^{2}$ are independent random variables,
> with $\overline{X} \sim \mathcal{N}(\mu,\sigma^{2} /n)$, and $\frac{n-1}{\sigma^{2}}S^{2} \sim \chi^{2}_{n-1}$.
> Then we have $\frac{\overline{X} - \mu}{S /\sqrt{n }} \sim t_{n-1}$.

- [!] This independence of $\overline{X}$ and $S^2$ is a unique property of the normal distribution.


</div></div>
         | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done  

# Normal Distribution

A [[Random Variable\|Random Variable]] is said to be normally distributed if it has

- Notation
    - $\mathcal{N}(\mu, \sigma^{2})$ ^nota
- Parameters
    - $\mu\in\R, \sigma^{2}\in\R_{+}$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$ ^pdf
    - $(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$
- [[Expectation\|Mean]]
    - $\mu$ ^mean
- [[Variance\|Variance]]
    - $\sigma^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $e^{\mu t + \sigma^{2}t^{2} /2}$ ^mgf

Because of the [[Central Limit Theorem\|Central Limit Theorem]], in practice, many random phenomena obey, at least approximately, a normal probability distribution.

## Properties

- The [[Affine Transformation\|Affine Transformation]] of a normal random variable $X$: $a + BX$ is also a normal random variable
- The sum of independent normal random variables is also a normal random variable
- Hence, if $X\sim \mathcal{N}(\mu,\sigma^{2})$, then $Z = (X - \mu) /\sigma$ is normal with mean 0 and variance 1; $Z$ is said to have a ==standard== or ==unit== normal distribution
    - We write the PDF of a standard normal distribution $\Phi$
- **Symmetry:** $\Phi(-x) = 1 - \Phi(x)$

## Sample Mean and Sample Variance

> [!thm]
>
> If $\{ X _i \}_{i=1}^{n}$ is a sample from a normal population having mean $\mu$ and variance $\sigma^{2}$, then $\overline{X}$ and $S^{2}$ are independent random variables,
> with $\overline{X} \sim \mathcal{N}(\mu,\sigma^{2} /n)$, and $\frac{n-1}{\sigma^{2}}S^{2} \sim \chi^{2}_{n-1}$.
> Then we have $\frac{\overline{X} - \mu}{S /\sqrt{n }} \sim t_{n-1}$.

- [!] This independence of $\overline{X}$ and $S^2$ is a unique property of the normal distribution.


</div></div>
         | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done  

# Normal Distribution

A [[Random Variable\|Random Variable]] is said to be normally distributed if it has

- Notation
    - $\mathcal{N}(\mu, \sigma^{2})$ ^nota
- Parameters
    - $\mu\in\R, \sigma^{2}\in\R_{+}$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$ ^pdf
    - $(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$
- [[Expectation\|Mean]]
    - $\mu$ ^mean
- [[Variance\|Variance]]
    - $\sigma^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $e^{\mu t + \sigma^{2}t^{2} /2}$ ^mgf

Because of the [[Central Limit Theorem\|Central Limit Theorem]], in practice, many random phenomena obey, at least approximately, a normal probability distribution.

## Properties

- The [[Affine Transformation\|Affine Transformation]] of a normal random variable $X$: $a + BX$ is also a normal random variable
- The sum of independent normal random variables is also a normal random variable
- Hence, if $X\sim \mathcal{N}(\mu,\sigma^{2})$, then $Z = (X - \mu) /\sigma$ is normal with mean 0 and variance 1; $Z$ is said to have a ==standard== or ==unit== normal distribution
    - We write the PDF of a standard normal distribution $\Phi$
- **Symmetry:** $\Phi(-x) = 1 - \Phi(x)$

## Sample Mean and Sample Variance

> [!thm]
>
> If $\{ X _i \}_{i=1}^{n}$ is a sample from a normal population having mean $\mu$ and variance $\sigma^{2}$, then $\overline{X}$ and $S^{2}$ are independent random variables,
> with $\overline{X} \sim \mathcal{N}(\mu,\sigma^{2} /n)$, and $\frac{n-1}{\sigma^{2}}S^{2} \sim \chi^{2}_{n-1}$.
> Then we have $\frac{\overline{X} - \mu}{S /\sqrt{n }} \sim t_{n-1}$.

- [!] This independence of $\overline{X}$ and $S^2$ is a unique property of the normal distribution.


</div></div>
         | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done  

# Normal Distribution

A [[Random Variable\|Random Variable]] is said to be normally distributed if it has

- Notation
    - $\mathcal{N}(\mu, \sigma^{2})$ ^nota
- Parameters
    - $\mu\in\R, \sigma^{2}\in\R_{+}$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$ ^pdf
    - $(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$
- [[Expectation\|Mean]]
    - $\mu$ ^mean
- [[Variance\|Variance]]
    - $\sigma^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $e^{\mu t + \sigma^{2}t^{2} /2}$ ^mgf

Because of the [[Central Limit Theorem\|Central Limit Theorem]], in practice, many random phenomena obey, at least approximately, a normal probability distribution.

## Properties

- The [[Affine Transformation\|Affine Transformation]] of a normal random variable $X$: $a + BX$ is also a normal random variable
- The sum of independent normal random variables is also a normal random variable
- Hence, if $X\sim \mathcal{N}(\mu,\sigma^{2})$, then $Z = (X - \mu) /\sigma$ is normal with mean 0 and variance 1; $Z$ is said to have a ==standard== or ==unit== normal distribution
    - We write the PDF of a standard normal distribution $\Phi$
- **Symmetry:** $\Phi(-x) = 1 - \Phi(x)$

## Sample Mean and Sample Variance

> [!thm]
>
> If $\{ X _i \}_{i=1}^{n}$ is a sample from a normal population having mean $\mu$ and variance $\sigma^{2}$, then $\overline{X}$ and $S^{2}$ are independent random variables,
> with $\overline{X} \sim \mathcal{N}(\mu,\sigma^{2} /n)$, and $\frac{n-1}{\sigma^{2}}S^{2} \sim \chi^{2}_{n-1}$.
> Then we have $\frac{\overline{X} - \mu}{S /\sqrt{n }} \sim t_{n-1}$.

- [!] This independence of $\overline{X}$ and $S^2$ is a unique property of the normal distribution.


</div></div>
         | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-
sup:: [[Probability Theory\|Probability Theory]]  
state:: done  

# Normal Distribution

A [[Random Variable\|Random Variable]] is said to be normally distributed if it has

- Notation
    - $\mathcal{N}(\mu, \sigma^{2})$ ^nota
- Parameters
    - $\mu\in\R, \sigma^{2}\in\R_{+}$ ^para
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$ ^pdf
    - $(2 \pi)^{-k / 2} \operatorname{det}(\boldsymbol{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$
- [[Expectation\|Mean]]
    - $\mu$ ^mean
- [[Variance\|Variance]]
    - $\sigma^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $e^{\mu t + \sigma^{2}t^{2} /2}$ ^mgf

Because of the [[Central Limit Theorem\|Central Limit Theorem]], in practice, many random phenomena obey, at least approximately, a normal probability distribution.

## Properties

- The [[Affine Transformation\|Affine Transformation]] of a normal random variable $X$: $a + BX$ is also a normal random variable
- The sum of independent normal random variables is also a normal random variable
- Hence, if $X\sim \mathcal{N}(\mu,\sigma^{2})$, then $Z = (X - \mu) /\sigma$ is normal with mean 0 and variance 1; $Z$ is said to have a ==standard== or ==unit== normal distribution
    - We write the PDF of a standard normal distribution $\Phi$
- **Symmetry:** $\Phi(-x) = 1 - \Phi(x)$

## Sample Mean and Sample Variance

> [!thm]
>
> If $\{ X _i \}_{i=1}^{n}$ is a sample from a normal population having mean $\mu$ and variance $\sigma^{2}$, then $\overline{X}$ and $S^{2}$ are independent random variables,
> with $\overline{X} \sim \mathcal{N}(\mu,\sigma^{2} /n)$, and $\frac{n-1}{\sigma^{2}}S^{2} \sim \chi^{2}_{n-1}$.
> Then we have $\frac{\overline{X} - \mu}{S /\sqrt{n }} \sim t_{n-1}$.

- [!] This independence of $\overline{X}$ and $S^2$ is a unique property of the normal distribution.


</div></div>
      |
| [[Gamma Distribution\|Gamma Distribution]]          | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Gamma Distribution

A continuous [[Random Variable\|Random Variable]] is of gamma distribution if

- Parameters
    - $\alpha, \lambda >0$ ^para
- [[Probability Density Function\|PDF]]
    - $f(x)=\begin{cases} \frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha -1}}{\Gamma(\alpha)},\quad & x \ge 0,\\0,& x<0 \end{cases}$ ^pdf
    - where $\Gamma$ is the [[Gamma Function\|Gamma Function]]
        - $\Gamma(\alpha) = \int _{0}^{\infty} e^{-y}y^{\alpha-1} \, dy$
        - $\Gamma(1) = 1$
        - $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$
        - $\Gamma(n) = (n-1)!$
- [[Expectation\|Mean]]
    - $\alpha /\lambda$ ^mean
- [[Variance\|Variance]]
    - $\alpha /\lambda^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\left( \frac{\lambda}{\lambda - t} \right)^{\alpha},\quad t <\lambda$ ^mgf


</div></div>
          | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Gamma Distribution

A continuous [[Random Variable\|Random Variable]] is of gamma distribution if

- Parameters
    - $\alpha, \lambda >0$ ^para
- [[Probability Density Function\|PDF]]
    - $f(x)=\begin{cases} \frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha -1}}{\Gamma(\alpha)},\quad & x \ge 0,\\0,& x<0 \end{cases}$ ^pdf
    - where $\Gamma$ is the [[Gamma Function\|Gamma Function]]
        - $\Gamma(\alpha) = \int _{0}^{\infty} e^{-y}y^{\alpha-1} \, dy$
        - $\Gamma(1) = 1$
        - $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$
        - $\Gamma(n) = (n-1)!$
- [[Expectation\|Mean]]
    - $\alpha /\lambda$ ^mean
- [[Variance\|Variance]]
    - $\alpha /\lambda^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\left( \frac{\lambda}{\lambda - t} \right)^{\alpha},\quad t <\lambda$ ^mgf


</div></div>
          | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Gamma Distribution

A continuous [[Random Variable\|Random Variable]] is of gamma distribution if

- Parameters
    - $\alpha, \lambda >0$ ^para
- [[Probability Density Function\|PDF]]
    - $f(x)=\begin{cases} \frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha -1}}{\Gamma(\alpha)},\quad & x \ge 0,\\0,& x<0 \end{cases}$ ^pdf
    - where $\Gamma$ is the [[Gamma Function\|Gamma Function]]
        - $\Gamma(\alpha) = \int _{0}^{\infty} e^{-y}y^{\alpha-1} \, dy$
        - $\Gamma(1) = 1$
        - $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$
        - $\Gamma(n) = (n-1)!$
- [[Expectation\|Mean]]
    - $\alpha /\lambda$ ^mean
- [[Variance\|Variance]]
    - $\alpha /\lambda^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\left( \frac{\lambda}{\lambda - t} \right)^{\alpha},\quad t <\lambda$ ^mgf


</div></div>
          | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Gamma Distribution

A continuous [[Random Variable\|Random Variable]] is of gamma distribution if

- Parameters
    - $\alpha, \lambda >0$ ^para
- [[Probability Density Function\|PDF]]
    - $f(x)=\begin{cases} \frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha -1}}{\Gamma(\alpha)},\quad & x \ge 0,\\0,& x<0 \end{cases}$ ^pdf
    - where $\Gamma$ is the [[Gamma Function\|Gamma Function]]
        - $\Gamma(\alpha) = \int _{0}^{\infty} e^{-y}y^{\alpha-1} \, dy$
        - $\Gamma(1) = 1$
        - $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$
        - $\Gamma(n) = (n-1)!$
- [[Expectation\|Mean]]
    - $\alpha /\lambda$ ^mean
- [[Variance\|Variance]]
    - $\alpha /\lambda^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\left( \frac{\lambda}{\lambda - t} \right)^{\alpha},\quad t <\lambda$ ^mgf


</div></div>
          | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Gamma Distribution

A continuous [[Random Variable\|Random Variable]] is of gamma distribution if

- Parameters
    - $\alpha, \lambda >0$ ^para
- [[Probability Density Function\|PDF]]
    - $f(x)=\begin{cases} \frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha -1}}{\Gamma(\alpha)},\quad & x \ge 0,\\0,& x<0 \end{cases}$ ^pdf
    - where $\Gamma$ is the [[Gamma Function\|Gamma Function]]
        - $\Gamma(\alpha) = \int _{0}^{\infty} e^{-y}y^{\alpha-1} \, dy$
        - $\Gamma(1) = 1$
        - $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$
        - $\Gamma(n) = (n-1)!$
- [[Expectation\|Mean]]
    - $\alpha /\lambda$ ^mean
- [[Variance\|Variance]]
    - $\alpha /\lambda^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\left( \frac{\lambda}{\lambda - t} \right)^{\alpha},\quad t <\lambda$ ^mgf


</div></div>
       |
| [[Chi-Square Distribution\|Chi-Square Distribution]]     | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline n-link naked

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Chi-Square Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/642px-Chi-square_pdf.svg.png)

If $Z_1,\dots,Z_n$ are IID standard [[Normal Distribution\|normal random variables]], then
$
X = \sum^{n}_{i=1} Z_{i}^{2}
$
is said to have a **chi-square** distribution with $n$ degrees of freedom.

By the uniqueness of [[Moment Generating Function\|MGF]], chi-square distribution with $n$ degrees of freedom is [[Gamma Distribution\|Gamma Distribution]] with parameter $(n/2,1/2)$.

- Notation
    - $\chi _{n}^{2}$ ^nota
- Parameters
    - $n$ ^para
- [[Moment Generating Function\|MGF]]
    - $(1-2t)^{-n/2}$ ^mgf
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x) = \frac{e^{-x/2}(x/2)^{n /2 -1}}{2\Gamma(n /2)}, \quad x\ge 0$ ^pdf
- [[Expectation\|Mean]]
    - $n$ ^mean
- [[Variance\|Variance]]
    - $2n$ ^var


</div></div>
 | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Chi-Square Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/642px-Chi-square_pdf.svg.png)

If $Z_1,\dots,Z_n$ are IID standard [[Normal Distribution\|normal random variables]], then
$
X = \sum^{n}_{i=1} Z_{i}^{2}
$
is said to have a **chi-square** distribution with $n$ degrees of freedom.

By the uniqueness of [[Moment Generating Function\|MGF]], chi-square distribution with $n$ degrees of freedom is [[Gamma Distribution\|Gamma Distribution]] with parameter $(n/2,1/2)$.

- Notation
    - $\chi _{n}^{2}$ ^nota
- Parameters
    - $n$ ^para
- [[Moment Generating Function\|MGF]]
    - $(1-2t)^{-n/2}$ ^mgf
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x) = \frac{e^{-x/2}(x/2)^{n /2 -1}}{2\Gamma(n /2)}, \quad x\ge 0$ ^pdf
- [[Expectation\|Mean]]
    - $n$ ^mean
- [[Variance\|Variance]]
    - $2n$ ^var


</div></div>
     | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Chi-Square Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/642px-Chi-square_pdf.svg.png)

If $Z_1,\dots,Z_n$ are IID standard [[Normal Distribution\|normal random variables]], then
$
X = \sum^{n}_{i=1} Z_{i}^{2}
$
is said to have a **chi-square** distribution with $n$ degrees of freedom.

By the uniqueness of [[Moment Generating Function\|MGF]], chi-square distribution with $n$ degrees of freedom is [[Gamma Distribution\|Gamma Distribution]] with parameter $(n/2,1/2)$.

- Notation
    - $\chi _{n}^{2}$ ^nota
- Parameters
    - $n$ ^para
- [[Moment Generating Function\|MGF]]
    - $(1-2t)^{-n/2}$ ^mgf
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x) = \frac{e^{-x/2}(x/2)^{n /2 -1}}{2\Gamma(n /2)}, \quad x\ge 0$ ^pdf
- [[Expectation\|Mean]]
    - $n$ ^mean
- [[Variance\|Variance]]
    - $2n$ ^var


</div></div>
     | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Chi-Square Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/642px-Chi-square_pdf.svg.png)

If $Z_1,\dots,Z_n$ are IID standard [[Normal Distribution\|normal random variables]], then
$
X = \sum^{n}_{i=1} Z_{i}^{2}
$
is said to have a **chi-square** distribution with $n$ degrees of freedom.

By the uniqueness of [[Moment Generating Function\|MGF]], chi-square distribution with $n$ degrees of freedom is [[Gamma Distribution\|Gamma Distribution]] with parameter $(n/2,1/2)$.

- Notation
    - $\chi _{n}^{2}$ ^nota
- Parameters
    - $n$ ^para
- [[Moment Generating Function\|MGF]]
    - $(1-2t)^{-n/2}$ ^mgf
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x) = \frac{e^{-x/2}(x/2)^{n /2 -1}}{2\Gamma(n /2)}, \quad x\ge 0$ ^pdf
- [[Expectation\|Mean]]
    - $n$ ^mean
- [[Variance\|Variance]]
    - $2n$ ^var


</div></div>
     | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Chi-Square Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/642px-Chi-square_pdf.svg.png)

If $Z_1,\dots,Z_n$ are IID standard [[Normal Distribution\|normal random variables]], then
$
X = \sum^{n}_{i=1} Z_{i}^{2}
$
is said to have a **chi-square** distribution with $n$ degrees of freedom.

By the uniqueness of [[Moment Generating Function\|MGF]], chi-square distribution with $n$ degrees of freedom is [[Gamma Distribution\|Gamma Distribution]] with parameter $(n/2,1/2)$.

- Notation
    - $\chi _{n}^{2}$ ^nota
- Parameters
    - $n$ ^para
- [[Moment Generating Function\|MGF]]
    - $(1-2t)^{-n/2}$ ^mgf
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x) = \frac{e^{-x/2}(x/2)^{n /2 -1}}{2\Gamma(n /2)}, \quad x\ge 0$ ^pdf
- [[Expectation\|Mean]]
    - $n$ ^mean
- [[Variance\|Variance]]
    - $2n$ ^var


</div></div>
     | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Chi-Square Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/642px-Chi-square_pdf.svg.png)

If $Z_1,\dots,Z_n$ are IID standard [[Normal Distribution\|normal random variables]], then
$
X = \sum^{n}_{i=1} Z_{i}^{2}
$
is said to have a **chi-square** distribution with $n$ degrees of freedom.

By the uniqueness of [[Moment Generating Function\|MGF]], chi-square distribution with $n$ degrees of freedom is [[Gamma Distribution\|Gamma Distribution]] with parameter $(n/2,1/2)$.

- Notation
    - $\chi _{n}^{2}$ ^nota
- Parameters
    - $n$ ^para
- [[Moment Generating Function\|MGF]]
    - $(1-2t)^{-n/2}$ ^mgf
- [[Probability Density Function\|PDF]]
    - $\displaystyle f(x) = \frac{e^{-x/2}(x/2)^{n /2 -1}}{2\Gamma(n /2)}, \quad x\ge 0$ ^pdf
- [[Expectation\|Mean]]
    - $n$ ^mean
- [[Variance\|Variance]]
    - $2n$ ^var


</div></div>
  |
| [[t Distribution\|t Distribution]]              | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# t-Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/650px-Student_t_pdf.svg.png)

While we can show that the sample [[Variance\|Variance]] of [[Normal Distribution\|Normal Distribution]] samples is of [[Chi-Square Distribution\|Chi-Square Distribution]], it is convenient to define the distribution for the following [[Random Variable\|Random Variable]]
$
T_n = \frac{Z}{\sqrt{ \chi _{n}^{2} /n }},
$
where $Z \sim \mathcal{N}(0,1)$. $T_n$ is said to have a **t-distribution** with $n$ degrees of freedom.

By the [[Weak Law of Large Numbers\|Weak Law of Large Numbers]], for large $n$, $T_n \approx Z \sim \mathcal{N}(0,1)$ (but always with larger [[Variance\|Variance]]).

- Parameter
    - $n \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - 0 ^mean
- [[Variance\|Variance]]

- $\frac{n}{n-2}$ ^var
- [[Probability Density Function\|PDF]]
    - $\displaystyle\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}$ ^pdf
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# t-Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/650px-Student_t_pdf.svg.png)

While we can show that the sample [[Variance\|Variance]] of [[Normal Distribution\|Normal Distribution]] samples is of [[Chi-Square Distribution\|Chi-Square Distribution]], it is convenient to define the distribution for the following [[Random Variable\|Random Variable]]
$
T_n = \frac{Z}{\sqrt{ \chi _{n}^{2} /n }},
$
where $Z \sim \mathcal{N}(0,1)$. $T_n$ is said to have a **t-distribution** with $n$ degrees of freedom.

By the [[Weak Law of Large Numbers\|Weak Law of Large Numbers]], for large $n$, $T_n \approx Z \sim \mathcal{N}(0,1)$ (but always with larger [[Variance\|Variance]]).

- Parameter
    - $n \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - 0 ^mean
- [[Variance\|Variance]]

- $\frac{n}{n-2}$ ^var
- [[Probability Density Function\|PDF]]
    - $\displaystyle\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}$ ^pdf
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# t-Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/650px-Student_t_pdf.svg.png)

While we can show that the sample [[Variance\|Variance]] of [[Normal Distribution\|Normal Distribution]] samples is of [[Chi-Square Distribution\|Chi-Square Distribution]], it is convenient to define the distribution for the following [[Random Variable\|Random Variable]]
$
T_n = \frac{Z}{\sqrt{ \chi _{n}^{2} /n }},
$
where $Z \sim \mathcal{N}(0,1)$. $T_n$ is said to have a **t-distribution** with $n$ degrees of freedom.

By the [[Weak Law of Large Numbers\|Weak Law of Large Numbers]], for large $n$, $T_n \approx Z \sim \mathcal{N}(0,1)$ (but always with larger [[Variance\|Variance]]).

- Parameter
    - $n \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - 0 ^mean
- [[Variance\|Variance]]

- $\frac{n}{n-2}$ ^var
- [[Probability Density Function\|PDF]]
    - $\displaystyle\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}$ ^pdf
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# t-Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/650px-Student_t_pdf.svg.png)

While we can show that the sample [[Variance\|Variance]] of [[Normal Distribution\|Normal Distribution]] samples is of [[Chi-Square Distribution\|Chi-Square Distribution]], it is convenient to define the distribution for the following [[Random Variable\|Random Variable]]
$
T_n = \frac{Z}{\sqrt{ \chi _{n}^{2} /n }},
$
where $Z \sim \mathcal{N}(0,1)$. $T_n$ is said to have a **t-distribution** with $n$ degrees of freedom.

By the [[Weak Law of Large Numbers\|Weak Law of Large Numbers]], for large $n$, $T_n \approx Z \sim \mathcal{N}(0,1)$ (but always with larger [[Variance\|Variance]]).

- Parameter
    - $n \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - 0 ^mean
- [[Variance\|Variance]]

- $\frac{n}{n-2}$ ^var
- [[Probability Density Function\|PDF]]
    - $\displaystyle\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}$ ^pdf
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# t-Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/650px-Student_t_pdf.svg.png)

While we can show that the sample [[Variance\|Variance]] of [[Normal Distribution\|Normal Distribution]] samples is of [[Chi-Square Distribution\|Chi-Square Distribution]], it is convenient to define the distribution for the following [[Random Variable\|Random Variable]]
$
T_n = \frac{Z}{\sqrt{ \chi _{n}^{2} /n }},
$
where $Z \sim \mathcal{N}(0,1)$. $T_n$ is said to have a **t-distribution** with $n$ degrees of freedom.

By the [[Weak Law of Large Numbers\|Weak Law of Large Numbers]], for large $n$, $T_n \approx Z \sim \mathcal{N}(0,1)$ (but always with larger [[Variance\|Variance]]).

- Parameter
    - $n \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - 0 ^mean
- [[Variance\|Variance]]

- $\frac{n}{n-2}$ ^var
- [[Probability Density Function\|PDF]]
    - $\displaystyle\frac{\Gamma \left(\frac{n+1}{2} \right)} {\sqrt{n\pi}\,\Gamma \left(\frac{n}{2} \right)} \left(1+\frac{x^2}{n} \right)^{-\frac{n+1}{2}}$ ^pdf
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
           |
| [[F Distribution\|F Distribution]]              | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# F Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/650px-F-distribution_pdf.svg.png)

The random variable defined by
$
F_{n,m} = \frac{\chi _{n}^{2}/n}{\chi _{m}^{2}/m}
$
is said to have an **F-distribution** with $n,m$ degrees of freedom.

- Parameters
    - $n,m \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - $m/(m+2)$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle \frac{2m^{2}(m+n -2)}{n(m-2)^{2}(m-4)}$ ^var
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | /                                                       | /                                                          | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# F Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/650px-F-distribution_pdf.svg.png)

The random variable defined by
$
F_{n,m} = \frac{\chi _{n}^{2}/n}{\chi _{m}^{2}/m}
$
is said to have an **F-distribution** with $n,m$ degrees of freedom.

- Parameters
    - $n,m \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - $m/(m+2)$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle \frac{2m^{2}(m+n -2)}{n(m-2)^{2}(m-4)}$ ^var
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# F Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/650px-F-distribution_pdf.svg.png)

The random variable defined by
$
F_{n,m} = \frac{\chi _{n}^{2}/n}{\chi _{m}^{2}/m}
$
is said to have an **F-distribution** with $n,m$ degrees of freedom.

- Parameters
    - $n,m \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - $m/(m+2)$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle \frac{2m^{2}(m+n -2)}{n(m-2)^{2}(m-4)}$ ^var
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
              | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# F Distribution

![pdf](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/650px-F-distribution_pdf.svg.png)

The random variable defined by
$
F_{n,m} = \frac{\chi _{n}^{2}/n}{\chi _{m}^{2}/m}
$
is said to have an **F-distribution** with $n,m$ degrees of freedom.

- Parameters
    - $n,m \in \mathbb{N}$ ^para
- [[Expectation\|Mean]]
    - $m/(m+2)$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle \frac{2m^{2}(m+n -2)}{n(m-2)^{2}(m-4)}$ ^var
- [[Moment Generating Function\|MGF]]
    - *Undefined* ^mgf


</div></div>
           |
| [[Geometric Distribution\|Geometric Distribution]]      | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Geometric Distribution

For a [[Bernoulli Trial\|Bernoulli Trial]], let $X$ be the number of tests when we first meet 1. Then $X$ is said to have a **geometric distribution**.

- Parameter
    - $p\in [0,1]$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(n) = 1 - q^{n}$ ^cdf
- [[Probability Mass Function\|PMF]]
    - $p(n) = pq^{n-1}$ ^pdf
- [[Expectation\|Mean]]
    - $1 /p$ ^mean
- [[Variance\|Variance]]
    - $q /p^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\frac{pe^{t}}{1-qe^{t}}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Geometric Distribution

For a [[Bernoulli Trial\|Bernoulli Trial]], let $X$ be the number of tests when we first meet 1. Then $X$ is said to have a **geometric distribution**.

- Parameter
    - $p\in [0,1]$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(n) = 1 - q^{n}$ ^cdf
- [[Probability Mass Function\|PMF]]
    - $p(n) = pq^{n-1}$ ^pdf
- [[Expectation\|Mean]]
    - $1 /p$ ^mean
- [[Variance\|Variance]]
    - $q /p^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\frac{pe^{t}}{1-qe^{t}}$ ^mgf


</div></div>
   | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Geometric Distribution

For a [[Bernoulli Trial\|Bernoulli Trial]], let $X$ be the number of tests when we first meet 1. Then $X$ is said to have a **geometric distribution**.

- Parameter
    - $p\in [0,1]$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(n) = 1 - q^{n}$ ^cdf
- [[Probability Mass Function\|PMF]]
    - $p(n) = pq^{n-1}$ ^pdf
- [[Expectation\|Mean]]
    - $1 /p$ ^mean
- [[Variance\|Variance]]
    - $q /p^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\frac{pe^{t}}{1-qe^{t}}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Geometric Distribution

For a [[Bernoulli Trial\|Bernoulli Trial]], let $X$ be the number of tests when we first meet 1. Then $X$ is said to have a **geometric distribution**.

- Parameter
    - $p\in [0,1]$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(n) = 1 - q^{n}$ ^cdf
- [[Probability Mass Function\|PMF]]
    - $p(n) = pq^{n-1}$ ^pdf
- [[Expectation\|Mean]]
    - $1 /p$ ^mean
- [[Variance\|Variance]]
    - $q /p^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\frac{pe^{t}}{1-qe^{t}}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Geometric Distribution

For a [[Bernoulli Trial\|Bernoulli Trial]], let $X$ be the number of tests when we first meet 1. Then $X$ is said to have a **geometric distribution**.

- Parameter
    - $p\in [0,1]$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(n) = 1 - q^{n}$ ^cdf
- [[Probability Mass Function\|PMF]]
    - $p(n) = pq^{n-1}$ ^pdf
- [[Expectation\|Mean]]
    - $1 /p$ ^mean
- [[Variance\|Variance]]
    - $q /p^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\frac{pe^{t}}{1-qe^{t}}$ ^mgf


</div></div>
      | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Geometric Distribution

For a [[Bernoulli Trial\|Bernoulli Trial]], let $X$ be the number of tests when we first meet 1. Then $X$ is said to have a **geometric distribution**.

- Parameter
    - $p\in [0,1]$ ^para
- [[Cumulative Distribution Function\|CDF]]
    - $F(n) = 1 - q^{n}$ ^cdf
- [[Probability Mass Function\|PMF]]
    - $p(n) = pq^{n-1}$ ^pdf
- [[Expectation\|Mean]]
    - $1 /p$ ^mean
- [[Variance\|Variance]]
    - $q /p^{2}$ ^var
- [[Moment Generating Function\|MGF]]
    - $\frac{pe^{t}}{1-qe^{t}}$ ^mgf


</div></div>
   |
| [[Hypergeometric Distribution\|Hypergeometric Distribution]] | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Hypergeometric Distribution

Consider a pool of $N$ machines which contains $M$ defect machines. Now $n$ machines are randomly selected, let $X$ be the number of defect machines selected. Then $X$ is said to have a **hypergeometric distribution**.

- Parameters
    - $N\in \mathbb{N}, M,n\in [N]$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(k) = \frac{{M \choose k}{N-M \choose n-k}}{{N \choose k}}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{nM}{N}$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle\frac{nM(N-n)(N-M)}{N^{2}(N-1)}$ ^var


</div></div>
 | /                                                       | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Hypergeometric Distribution

Consider a pool of $N$ machines which contains $M$ defect machines. Now $n$ machines are randomly selected, let $X$ be the number of defect machines selected. Then $X$ is said to have a **hypergeometric distribution**.

- Parameters
    - $N\in \mathbb{N}, M,n\in [N]$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(k) = \frac{{M \choose k}{N-M \choose n-k}}{{N \choose k}}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{nM}{N}$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle\frac{nM(N-n)(N-M)}{N^{2}(N-1)}$ ^var


</div></div>
 | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# naked inline n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Hypergeometric Distribution

Consider a pool of $N$ machines which contains $M$ defect machines. Now $n$ machines are randomly selected, let $X$ be the number of defect machines selected. Then $X$ is said to have a **hypergeometric distribution**.

- Parameters
    - $N\in \mathbb{N}, M,n\in [N]$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(k) = \frac{{M \choose k}{N-M \choose n-k}}{{N \choose k}}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{nM}{N}$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle\frac{nM(N-n)(N-M)}{N^{2}(N-1)}$ ^var


</div></div>
 | 
<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

$<div class="markdown-embed-title">

# inline naked n-link

</div>



> [!meta]-  
sup:: [[Probability Theory\|Probability Theory]]  
state:: done

# Hypergeometric Distribution

Consider a pool of $N$ machines which contains $M$ defect machines. Now $n$ machines are randomly selected, let $X$ be the number of defect machines selected. Then $X$ is said to have a **hypergeometric distribution**.

- Parameters
    - $N\in \mathbb{N}, M,n\in [N]$ ^para
- [[Probability Mass Function\|PMF]]
    - $\displaystyle p(k) = \frac{{M \choose k}{N-M \choose n-k}}{{N \choose k}}$ ^pdf
- [[Expectation\|Mean]]
    - $\frac{nM}{N}$ ^mean
- [[Variance\|Variance]]
    - $\displaystyle\frac{nM(N-n)(N-M)}{N^{2}(N-1)}$ ^var


</div></div>
 | /                                                       |
| [[Dirac Distribution\|Dirac Distribution]]          |                                                         |                                                             |                                                         |                                                            |                                                             |                                                            |                                                         |
| [[Laplace Distribution\|Laplace Distribution]]        |                                                         |                                                             |                                                         |                                                            |                                                             |                                                            |                                                         |

## Axioms

Three axioms of a probability $P$ on a probability space $X$:

1. For any $A \subset X$, $P(A) \in [0,1]$
2. $P(\varnothing) = 0$ and $P(X) = 1$
3. For any countable disjoint $\{ X _i \}$, $P(\bigsqcup_{i}X _i) = \sum_{i}P(X _i)$

Actually, $P(\varnothing) = 0$ can be derived by other axioms:
$$
P(\varnothing) + P(X) = P(\varnothing \sqcup X) = P(X) = 1.
$$
